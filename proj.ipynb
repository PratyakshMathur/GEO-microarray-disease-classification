{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6baeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q GEOparse scanpy anndata scikit-learn matplotlib seaborn pandas numpy joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ecc1d",
   "metadata": {},
   "source": [
    "# Breast cancer ML pipeline\n",
    "\n",
    "This notebook splits the original large cell into modular cells and adds caching: if merged outputs exist in `project_output`, the notebook will load them instead of re-downloading GSE datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5993abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and global settings\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GEO parsing and batch correction libraries\n",
    "import GEOparse\n",
    "import scanpy as sc\n",
    "\n",
    "# sklearn: selection, models, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def ensure_dir(p):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def infer_label_from_gsm(gsm: GEOparse.GSM) -> str:\n",
    "    \"\"\"Try to infer 'cancer' or 'normal' from GSM metadata fields heuristically.\"\"\"\n",
    "    text_fields = []\n",
    "    for key in [\"characteristics_ch1\", \"source_name_ch1\", \"description\", \"title\"]:\n",
    "        val = gsm.metadata.get(key)\n",
    "        if val:\n",
    "            if isinstance(val, list):\n",
    "                text_fields.extend(val)\n",
    "            else:\n",
    "                text_fields.append(str(val))\n",
    "    combined = \" \".join([str(x).lower() for x in text_fields])\n",
    "    if re.search(r\"normal|healthy|control|adjacent|non[-\\s]?tumor|non[-\\s]?tumour\", combined):\n",
    "        return \"normal\"\n",
    "    if re.search(r\"tumor|tumour|cancer|carcinoma|brca|malignant|diseased\", combined):\n",
    "        return \"cancer\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download & extract single GSE (keeps GEOparse caching behavior)\n",
    "def download_and_extract_gse(gse_id, outdir=\"geo_raw\"):\n",
    "    ensure_dir(outdir)\n",
    "    print(f\"Fetching {gse_id} (GEOparse will reuse cached files in {outdir} if present)...\")\n",
    "    gse = GEOparse.get_GEO(geo=gse_id, destdir=outdir, silent=True)\n",
    "    # Try series matrix table first\n",
    "    if hasattr(gse, \"table\") and isinstance(gse.table, pd.DataFrame) and not gse.table.empty:\n",
    "        df = gse.table.copy()\n",
    "        sample_cols = [c for c in df.columns if c.startswith(\"GSM\")]\n",
    "        if sample_cols:\n",
    "            proc = df[sample_cols].transpose()\n",
    "            proc.index.name = \"sample\"\n",
    "            proc.columns = df.iloc[:, 0].values\n",
    "            proc = proc.apply(pd.to_numeric, errors='coerce')\n",
    "            sample_meta = []\n",
    "            for gsm_id in proc.index:\n",
    "                gsm = gse.gsms.get(gsm_id)\n",
    "                sample_meta.append({\"sample\": gsm_id, \"label\": infer_label_from_gsm(gsm), **{k: (v if not isinstance(v, list) else \";\".join(v)) for k,v in gsm.metadata.items()}})\n",
    "            meta = pd.DataFrame(sample_meta).set_index(\"sample\")\n",
    "            return proc, meta\n",
    "    # Fallback: iterate GSM tables\n",
    "    exprs = {}\n",
    "    meta_rows = {}\n",
    "    for gsm_id, gsm in gse.gsms.items():\n",
    "        if hasattr(gsm, \"table\") and isinstance(gsm.table, pd.DataFrame) and not gsm.table.empty:\n",
    "            tbl = gsm.table.copy()\n",
    "            val_col = None\n",
    "            for c in tbl.columns:\n",
    "                if c.lower() in [\"value\", \"exprs\", \"intensity\", \"signal\", \"log2\"]:\n",
    "                    val_col = c\n",
    "                    break\n",
    "            if val_col is None:\n",
    "                val_col = tbl.columns[-1]\n",
    "            idx = None\n",
    "            for c in tbl.columns:\n",
    "                if c.lower() in [\"id\", \"id_ref\", \"probe_id\", \"gene_id\"]:\n",
    "                    idx = c\n",
    "                    break\n",
    "            if idx is None:\n",
    "                idx = tbl.columns[0]\n",
    "            s = tbl.set_index(idx)[val_col]\n",
    "            exprs[gsm_id] = s\n",
    "            meta_rows[gsm_id] = {\"label\": infer_label_from_gsm(gsm), **{k: (v if not isinstance(v, list) else \";\".join(v)) for k,v in gsm.metadata.items()}}\n",
    "    if not exprs:\n",
    "        raise RuntimeError(f\"No expression tables found for {gse_id} using GEOparse.\")\n",
    "    df_all = pd.concat(exprs, axis=1)\n",
    "    if isinstance(df_all.columns, pd.MultiIndex):\n",
    "        df_all.columns = [c[0] for c in df_all.columns]\n",
    "    proc = df_all.transpose().astype(float)\n",
    "    meta = pd.DataFrame.from_dict(meta_rows, orient='index')\n",
    "    proc.index.name = \"sample\"\n",
    "    return proc, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e11843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build merged dataset from multiple GSEs\n",
    "def build_merged_dataset(gse_list, outdir=\"geo_raw\", manual_label_csv=None):\n",
    "    exprs = []\n",
    "    metas = []\n",
    "    for gse in gse_list:\n",
    "        X, meta = download_and_extract_gse(gse, outdir=outdir)\n",
    "        batch_name = gse\n",
    "        meta = meta.copy()\n",
    "        meta[\"batch\"] = batch_name\n",
    "        meta.index = meta.index.astype(str)\n",
    "        X.index = X.index.astype(str)\n",
    "        exprs.append(X)\n",
    "        metas.append(meta)\n",
    "    # Align columns - intersection preferred\n",
    "    common_genes = set.intersection(*[set(df.columns) for df in exprs])\n",
    "    if len(common_genes) < 50:\n",
    "        print(\"Intersection small; using union and filling missing values.\")\n",
    "        merged_X = pd.concat(exprs, axis=0, sort=True).fillna(np.nan)\n",
    "    else:\n",
    "        print(f\"Using intersection of genes: {len(common_genes)} genes.\")\n",
    "        merged_X = pd.concat([df.loc[:, sorted(common_genes)] for df in exprs], axis=0)\n",
    "    merged_meta = pd.concat(metas, axis=0)\n",
    "    # Try manual labels if provided\n",
    "    if manual_label_csv and os.path.exists(manual_label_csv):\n",
    "        manual = pd.read_csv(manual_label_csv)\n",
    "        manual.index = manual['sample'].astype(str)\n",
    "        merged_meta.loc[manual.index, \"label\"] = manual['GROUP'].values\n",
    "    # Drop unlabeled\n",
    "    if \"label\" not in merged_meta.columns:\n",
    "        merged_meta[\"label\"] = None\n",
    "    n_unlabeled = merged_meta[\"label\"].isnull().sum()\n",
    "    if n_unlabeled > 0:\n",
    "        print(f\"Warning: {n_unlabeled} samples remain unlabeled. Dropping them.\")\n",
    "        keep = merged_meta[~merged_meta[\"label\"].isnull()].index\n",
    "        merged_meta = merged_meta.loc[keep]\n",
    "        merged_X = merged_X.loc[keep]\n",
    "    merged_meta[\"label\"] = merged_meta[\"label\"].astype(str).str.lower().replace({\"tumor\": \"cancer\", \"tumour\": \"cancer\", \"brca\": \"cancer\", \"control\": \"normal\", \"adjacent normal\": \"normal\"})\n",
    "    merged_meta = merged_meta[merged_meta[\"label\"].isin([\"cancer\", \"normal\"]) ]\n",
    "    merged_X = merged_X.loc[merged_meta.index]\n",
    "    return merged_X, merged_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and batch correction\n",
    "def preprocess_and_batch_correct(X_df: pd.DataFrame, meta_df: pd.DataFrame, log_transform=True):\n",
    "    X = X_df.copy()\n",
    "    X.columns = X.columns.astype(str)\n",
    "    X = X.fillna(X.median(axis=0))\n",
    "    if log_transform:\n",
    "        X = np.log1p(X)\n",
    "    \n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\n",
    "    adata = sc.AnnData(X_scaled)\n",
    "    adata.obs[\"batch\"] = meta_df.loc[adata.obs_names, \"batch\"].values\n",
    "    sc.pp.combat(adata, key=\"batch\")\n",
    "    X_corrected = pd.DataFrame(adata.X, index=X.index, columns=X.columns)\n",
    "    return X_corrected, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline(\n",
    "    gse_list,\n",
    "    geo_raw_dir=\"geo_raw\",\n",
    "    manual_label_csv=None,\n",
    "    save_processed=True,\n",
    "    processed_dir=\"processed_data\"\n",
    "    # results_dir=\"results_dir\"\n",
    "                                ):\n",
    "    \"\"\"\n",
    "    Loads processed data if available. Otherwise runs:\n",
    "    GEO download/merge → preprocess → batch correct → save\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # STEP 0: CHECK IF PROCESSED FILES ALREADY EXIST\n",
    "    # ---------------------------------------------------------------------\n",
    "    Xc_path = os.path.join(processed_dir, \"X_corrected.csv\")\n",
    "    meta_path = os.path.join(processed_dir, \"metadata.csv\")\n",
    "\n",
    "    if os.path.exists(Xc_path) and os.path.exists(meta_path):\n",
    "        print(\"\\n==============================\")\n",
    "        print(\"LOADING CACHED PROCESSED DATA\")\n",
    "        print(\"==============================\")\n",
    "\n",
    "        X_corrected = pd.read_csv(Xc_path, index_col=0)\n",
    "        meta = pd.read_csv(meta_path, index_col=0)\n",
    "\n",
    "        # Ensure label and batch exist\n",
    "        meta[\"label\"] = meta[\"label\"].astype(str)\n",
    "        X_corrected.columns = X_corrected.columns.astype(str)\n",
    "\n",
    "        # Dummy scaler (not needed later)\n",
    "        scaler = None\n",
    "        \n",
    "        return X_corrected, meta, scaler\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # OTHERWISE → RUN FULL PREPROCESSING PIPELINE\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"STEP 1: BUILD MERGED DATASET\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    X_raw, meta = build_merged_dataset(\n",
    "        gse_list=gse_list,\n",
    "        outdir=geo_raw_dir,\n",
    "        manual_label_csv=manual_label_csv\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMerged shape: {X_raw.shape}\")\n",
    "    print(meta['label'].value_counts())\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"STEP 2: PREPROCESS + BATCH CORRECT\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    X_corrected, scaler = preprocess_and_batch_correct(X_raw, meta)\n",
    "    \n",
    "    ensure_dir(processed_dir)\n",
    "    joblib.dump(scaler, os.path.join(processed_dir, \"scaler.joblib\"))\n",
    "    print(\"Scaler saved to:\", os.path.join(processed_dir, \"scaler.joblib\"))\n",
    "    if save_processed:\n",
    "        ensure_dir(processed_dir)\n",
    "        X_corrected.to_csv(Xc_path)\n",
    "        meta.to_csv(meta_path)\n",
    "        X_raw.to_csv(os.path.join(processed_dir, \"X_raw.csv\"))\n",
    "        print(f\"\\nProcessed data saved in: {processed_dir}/\")\n",
    "\n",
    "    return X_corrected, meta, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, var_thresh=0.01, max_features=500):\n",
    "    \"\"\"\n",
    "    SAFE FEATURE SELECTION PIPELINE\n",
    "    - Attempts variance threshold\n",
    "    - If removed all features → skip variance threshold\n",
    "    - Applies L1 logistic selection\n",
    "    - If still empty → fall back to top variance genes\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"STEP 3: FEATURE SELECTION\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # STEP 1: Variance Threshold\n",
    "    # ----------------------------\n",
    "    try:\n",
    "        vt = VarianceThreshold(threshold=var_thresh)\n",
    "        X_v = pd.DataFrame(\n",
    "            vt.fit_transform(X),\n",
    "            index=X.index,\n",
    "            columns=X.columns[vt.get_support()]\n",
    "        )\n",
    "        print(f\"Features after variance threshold: {X_v.shape[1]}\")\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"⚠️ Variance threshold removed ALL features. Skipping variance threshold.\")\n",
    "        X_v = X.copy()\n",
    "\n",
    "    # If still zero features\n",
    "    if X_v.shape[1] == 0:\n",
    "        print(\"⚠️ No features remain after threshold. Falling back to top variance genes.\")\n",
    "        var_series = X.var(axis=0).sort_values(ascending=False)\n",
    "        top_genes = var_series.head(200).index  # take 200 highest variance genes\n",
    "        X_v = X[top_genes]\n",
    "        print(f\"Selected top 200 variance genes: {len(top_genes)}\")\n",
    "\n",
    "    # Safety: keep column names strings\n",
    "    X_v.columns = X_v.columns.astype(str)\n",
    "\n",
    "    # ----------------------------\n",
    "    # STEP 2: L1 Logistic Selection\n",
    "    # ----------------------------\n",
    "\n",
    "    try:\n",
    "        lr = LogisticRegressionCV(\n",
    "            Cs=10,\n",
    "            cv=5,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "            scoring=\"roc_auc\",\n",
    "            max_iter=2000,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        lr.fit(X_v, y)\n",
    "\n",
    "        coef = np.abs(lr.coef_).ravel()\n",
    "        sel_mask = coef > 1e-6\n",
    "        selected = X_v.columns[sel_mask]\n",
    "        print(f\"L1 selected: {len(selected)}\")\n",
    "\n",
    "        # If L1 selects nothing → fallback\n",
    "        if len(selected) == 0:\n",
    "            print(\"⚠️ L1 selected ZERO features. Falling back to top variance genes.\")\n",
    "            var_series = X.var(axis=0).sort_values(ascending=False)\n",
    "            selected = var_series.head(max_features).index\n",
    "            X_sel = X[selected]\n",
    "            return X_sel, list(selected)\n",
    "\n",
    "        # Truncate to max_features if needed\n",
    "        if len(selected) > max_features:\n",
    "            ranks = np.argsort(-coef[sel_mask])\n",
    "            selected = selected[ranks[:max_features]]\n",
    "            print(f\"Truncated to top {max_features} L1 features.\")\n",
    "\n",
    "        X_sel = X_v[selected]\n",
    "        return X_sel, list(selected)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ L1 selection failed due to:\", e)\n",
    "        print(\"Fallback: Selecting top variance genes.\")\n",
    "\n",
    "        var_series = X.var(axis=0).sort_values(ascending=False)\n",
    "        selected = var_series.head(max_features).index\n",
    "        X_sel = X[selected]\n",
    "        return X_sel, list(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate(X, y, outdir=\"results\"):\n",
    "    ensure_dir(outdir)\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"STEP 4: TRAIN & EVALUATE MODELS\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "    classes = le.classes_\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_enc, stratify=y_enc, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    print(f\"Train samples: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "    # -------------------- Random Forest --------------------\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "    auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
    "    print(f\"RF ROC-AUC = {auc_rf:.4f}\")\n",
    "\n",
    "    # ------------------------- MLP --------------------------\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(512, 128),\n",
    "                        max_iter=500,\n",
    "                        random_state=RANDOM_STATE,\n",
    "                        early_stopping=True)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    y_proba_mlp = mlp.predict_proba(X_test)[:, 1]\n",
    "    y_pred_mlp = mlp.predict(X_test)\n",
    "    auc_mlp = roc_auc_score(y_test, y_proba_mlp)\n",
    "    print(f\"MLP ROC-AUC = {auc_mlp:.4f}\")\n",
    "\n",
    "    # Save models\n",
    "\n",
    "    joblib.dump(rf, os.path.join(outdir, \"rf_model.joblib\"))\n",
    "    joblib.dump(mlp, os.path.join(outdir, \"mlp_model.joblib\"))\n",
    "\n",
    "    # Reports\n",
    "    print(\"\\nRandom Forest classification report:\")\n",
    "    print(classification_report(y_test, y_pred_rf, target_names=classes))\n",
    "\n",
    "    print(\"\\nMLP classification report:\")\n",
    "    print(classification_report(y_test, y_pred_mlp, target_names=classes))\n",
    "\n",
    "    # ROC plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "    fpr_mlp, tpr_mlp, _ = roc_curve(y_test, y_proba_mlp)\n",
    "    plt.plot(fpr_rf, tpr_rf, label=f\"RF (AUC={auc_rf:.3f})\")\n",
    "    plt.plot(fpr_mlp, tpr_mlp, label=f\"MLP (AUC={auc_mlp:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(outdir, \"roc_curves.png\"), dpi=200)\n",
    "\n",
    "    # Confusion Matrix (RF)\n",
    "    cm = confusion_matrix(y_test, y_pred_rf)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(\"RF Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(outdir, \"rf_confusion_matrix.png\"), dpi=200)\n",
    "\n",
    "    # RF top features heatmap\n",
    "    importances = rf.feature_importances_\n",
    "    top_idx = np.argsort(importances)[-30:][::-1]\n",
    "    top_genes = X.columns[top_idx]\n",
    "\n",
    "    sns.clustermap(X[top_genes].T, z_score=0, cmap=\"vlag\", figsize=(8, 10))\n",
    "    plt.suptitle(\"Heatmap (Top 30 Features)\")\n",
    "    plt.savefig(os.path.join(outdir, \"top_features_heatmap.png\"), dpi=200)\n",
    "\n",
    "    return {\n",
    "        \"rf_auc\": auc_rf,\n",
    "        \"mlp_auc\": auc_mlp,\n",
    "        \"rf_model\": rf,\n",
    "        \"mlp_model\": mlp,\n",
    "        \"label_encoder\": le,\n",
    "        \"top_features\": list(top_genes)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_training_pipeline(\n",
    "    gse_list,\n",
    "    geo_raw_dir=\"geo_raw\",\n",
    "    manual_label_csv=None,\n",
    "    processed_dir=\"processed_data\",\n",
    "    results_dir=\"results\",\n",
    "    var_thresh=0.01,\n",
    "    max_features=500\n",
    "):\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\" FULL TRAINING PIPELINE \")\n",
    "    print(\"======================================\")\n",
    "\n",
    "    # Step 1+2 are cached\n",
    "    X_corrected, meta, scaler = run_full_pipeline(\n",
    "        gse_list=gse_list,\n",
    "        geo_raw_dir=geo_raw_dir,\n",
    "        manual_label_csv=manual_label_csv,\n",
    "        save_processed=True,\n",
    "        processed_dir=processed_dir\n",
    "        # results_dir=results_dir\n",
    "    )\n",
    "\n",
    "    # Step 3: Feature selection\n",
    "    y = meta[\"label\"]\n",
    "    X_selected, selected_genes = feature_selection(\n",
    "        X_corrected, y,\n",
    "        var_thresh=var_thresh,\n",
    "        max_features=max_features\n",
    "    )\n",
    "    ensure_dir(results_dir)\n",
    "    with open(os.path.join(results_dir, \"selected_genes.txt\"), \"w\") as f:\n",
    "        for gene in selected_genes:\n",
    "            f.write(f\"{gene}\\n\")\n",
    "    print(\"Selected genes saved to:\", os.path.join(results_dir, \"selected_genes.txt\"))\n",
    "\n",
    "    # Step 4: Train models\n",
    "    results = train_and_evaluate(\n",
    "        X_selected,\n",
    "        y,\n",
    "        outdir=results_dir\n",
    "    )\n",
    "\n",
    "    results[\"selected_genes\"] = selected_genes\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff52c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_ids = [\"GSE2034\", \"GSE15852\"]\n",
    "# \"GSE70947\", \"GSE42568\"\n",
    "results = run_full_training_pipeline(\n",
    "    gse_list=gse_ids,\n",
    "    geo_raw_dir=\"geo_raw\",\n",
    "    processed_dir=\"processed_data\",\n",
    "    results_dir=\"results\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
